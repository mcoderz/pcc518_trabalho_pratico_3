{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size = 5000\n",
    "embedding_dim = 100\n",
    "max_length = 200\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "#oov_tok = '<OOV>'\n",
    "#training_portion = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_list (file_name):\n",
    "    \"\"\"\n",
    "    Converte o CSV em duas lista distintas:\n",
    "    articles e labels\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    labels = []\n",
    "\n",
    "    with open(file_name, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            labels.append(row[1])\n",
    "            articles.append(row[3])\n",
    "    \n",
    "    return labels, articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4024\n",
      "4024\n"
     ]
    }
   ],
   "source": [
    "test_labels = csv_to_list('data_test.csv')[0]\n",
    "test_category = csv_to_list('data_test.csv')[1]\n",
    "\n",
    "print(len(test_category))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11413\n",
      "11413\n"
     ]
    }
   ],
   "source": [
    "training_labels = csv_to_list('data_training.csv')[0]\n",
    "training_category = csv_to_list('data_training.csv')[1]\n",
    "\n",
    "print(len(training_category))\n",
    "print(len(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11413, 200)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(training_category)\n",
    "\n",
    "training_word_index = tokenizer.word_index\n",
    "training_vocab_size=len(training_word_index)\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_category)\n",
    "traning_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(traning_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4024, 200)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(test_category)\n",
    "\n",
    "#test_word_index = tokenizer.word_index\n",
    "#test_vocab_size=len(test_word_index)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_category)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(test_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11413, 1)\n"
     ]
    }
   ],
   "source": [
    "label_tokenizer = Tokenizer(filters='!\"#$%&()*+,./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "label_tokenizer.fit_on_texts(training_labels)\n",
    "\n",
    "training_label_seq = np.array(label_tokenizer.texts_to_sequences(training_labels))\n",
    "\n",
    "print(training_label_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4024, 1)\n"
     ]
    }
   ],
   "source": [
    "label_tokenizer = Tokenizer(filters='!\"#$%&()*+,./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "label_tokenizer.fit_on_texts(test_labels)\n",
    "\n",
    "test_label_seq = np.array(label_tokenizer.texts_to_sequences(test_labels))\n",
    "\n",
    "print(test_label_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {};\n",
    "with open('glove/glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split();\n",
    "        word = values[0];\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs;\n",
    "\n",
    "embeddings_matrix = np.zeros((training_vocab_size+1, embedding_dim))\n",
    "for word, i in training_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31798\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 100)          3179800   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200)               160800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 92)                9292      \n",
      "=================================================================\n",
      "Total params: 3,369,992\n",
      "Trainable params: 190,192\n",
      "Non-trainable params: 3,179,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(training_vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    tf.keras.layers.Dense(92, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11413 samples\n",
      "11413/11413 - 542s - loss: 1.9224 - accuracy: 0.5207\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "history = model.fit(traning_padded, training_label_seq, epochs=num_epochs, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "efde34d10431e78146c56074b710b4f7e5e083279935c85f31bd0be7c32811bf"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('outro_um': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
